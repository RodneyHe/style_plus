{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.generator import G\n",
    "from model.id_encoder import IDEncoder\n",
    "from model.attr_encoder import AttrEncoder\n",
    "from model.stylegan import StyleGAN_G\n",
    "from model.model import Network\n",
    "from data_loader.data_loader import DataLoader\n",
    "import utils\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from pathlib import Path \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "id_model_path = \"./pretrained/vggface2.h5\"\n",
    "stylegan_G_synthesis_path = \"./pretrained/stylegan_G_256x256_synthesis/stylegan_G_256x256.h5\"\n",
    "landmarks_model_path = \"./pretrained/face_utils/keypoints\"\n",
    "face_detection_model_path = \"./pretrained/face_utils/detector\"\n",
    "arcface_model_path = \"./pretrained/arcface_weights/weights-b\"\n",
    "\n",
    "class Args(object):\n",
    "    def __init__(self):\n",
    "        self.resolution = 256\n",
    "        self.load_checkpoint = False\n",
    "        self.train = True\n",
    "        self.dataset_path = Path(\"./dataset\")\n",
    "        self.train_data_size = 50000\n",
    "        self.batch_size = 6\n",
    "        self.reals = False\n",
    "        self.test_real_attr = True\n",
    "        self.train_real_attr = False\n",
    "\n",
    "\n",
    "args = Args()\n",
    "g_optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "\n",
    "stylegan_G = StyleGAN_G(resolution=256, truncation_psi=0.7)\n",
    "stylegan_G.built = True\n",
    "stylegan_G.load_weights(stylegan_G_synthesis_path, by_name=True)\n",
    "\n",
    "# generator = G(args, id_model_path, stylegan_G, landmarks_model_path, face_detection_model_path, arcface_model_path)\n",
    "\n",
    "# z = tf.random.normal((6, 512))\n",
    "# sp = tf.zeros((6, 9984))\n",
    "# w = stylegan_G.model_mapping(z)\n",
    "# images, style_list = stylegan_G.model_synthesis([w, sp])\n",
    "# images = (images + 1) / 2\n",
    "\n",
    "# pixel_loss_func = tf.keras.losses.MeanAbsoluteError(tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "embedding_network = Network(args=args, id_net_path=id_model_path, base_generator=stylegan_G, phase=\"embedding\", \n",
    "                            landmarks_net_path=landmarks_model_path,\n",
    "                            face_detection_model_path=face_detection_model_path, \n",
    "                            test_id_net_path=arcface_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "import collections\n",
    "\n",
    "import face_alignment\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "\n",
    "\n",
    "faN = face_alignment.FaceAlignment(face_alignment.LandmarksType._3D, flip_input=False, device=\"cpu\")\n",
    "\n",
    "input = io.imread(\"./dataset/dataset_256/image/00000/00000.png\")\n",
    "preds = faN.get_landmarks(input)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(preds[:,0]), max(preds[:,1]), max(preds[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(preds[:,0]), min(preds[:,1]), min(preds[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_style = dict(marker='o', markersize=2, linestyle='-', lw=2)\n",
    "\n",
    "pred_type = collections.namedtuple('prediction_type', ['slice', 'color'])\n",
    "pred_types = {'face': pred_type(slice(0, 17), (0.682, 0.780, 0.909, 0.5)),\n",
    "              'eyebrow1': pred_type(slice(17, 22), (1.0, 0.498, 0.055, 0.4)),\n",
    "              'eyebrow2': pred_type(slice(22, 27), (1.0, 0.498, 0.055, 0.4)),\n",
    "              'nose': pred_type(slice(27, 31), (0.345, 0.239, 0.443, 0.4)),\n",
    "              'nostril': pred_type(slice(31, 36), (0.345, 0.239, 0.443, 0.4)),\n",
    "              'eye1': pred_type(slice(36, 42), (0.596, 0.875, 0.541, 0.3)),\n",
    "              'eye2': pred_type(slice(42, 48), (0.596, 0.875, 0.541, 0.3)),\n",
    "              'lips': pred_type(slice(48, 60), (0.596, 0.875, 0.541, 0.3)),\n",
    "              'teeth': pred_type(slice(60, 68), (0.596, 0.875, 0.541, 0.4))\n",
    "              }\n",
    "\n",
    "fig = plt.figure(figsize=plt.figaspect(.5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.imshow(input)\n",
    "\n",
    "for pred_type in pred_types.values():\n",
    "    ax.plot(preds[pred_type.slice, 0],\n",
    "            preds[pred_type.slice, 1],\n",
    "            color=pred_type.color, **plot_style)\n",
    "\n",
    "ax.axis('off')\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "surf = ax.scatter3D(preds[:, 2],\n",
    "                  preds[:, 0],\n",
    "                  -preds[:, 1] * 1.2,\n",
    "                  c='cyan',\n",
    "                  alpha=1.0,\n",
    "                  edgecolor='b')\n",
    "\n",
    "for pred_type in pred_types.values():\n",
    "    ax.plot3D(preds[pred_type.slice, 2],\n",
    "              preds[pred_type.slice, 0],\n",
    "              -preds[pred_type.slice, 1] * 1.2, color='blue')\n",
    "\n",
    "#ax.view_init(elev=45., azim=45.)\n",
    "#ax.set_zlim(ax.get_zlim()[::-1])\n",
    "ax.set_ylim(ax.get_ylim()[::-1])\n",
    "\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "initializer = tf.keras.initializers.Orthogonal()\n",
    "values = initializer(shape=(3, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable = tf.Variable(values)\n",
    "trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape(persistent=True) as g_tape:\n",
    "    trainable = tf.Variable(values)\n",
    "    x1 = trainable * 2.\n",
    "\n",
    "g_tape.gradient(x1, trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.Variable(values)[0], tf.Variable(values)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.tensordot(values[0], tf.transpose(values[1]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_img_list = []\n",
    "for i in range(6):\n",
    "    id_img_list.append(plt.imread(f\"./output/exp_11/weights/step_id_img{i}.png\")[None, ...])\n",
    "\n",
    "id_img = tf.concat(id_img_list, 0)\n",
    "\n",
    "attr_img_list = []\n",
    "for i in range(6):\n",
    "    attr_img_list.append(plt.imread(f\"./output/exp_11/weights/step_attr_img{i}.png\")[None, ...])\n",
    "\n",
    "attr_img = tf.concat(attr_img_list, 0)\n",
    "\n",
    "id_z_matching = np.load(\"./output/exp_11/weights/id_z_matching.npy\")\n",
    "id_z_matching = tf.concat(id_z_matching, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(id_img[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"./output/exp_11/weights/step_noise\", \"rb\") as f:\n",
    "    step_noise = pickle.load(f)\n",
    "\n",
    "with open(\"./output/exp_11/weights/step_grads\", \"rb\") as f:\n",
    "    step_grads = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stylegan_G.model_synthesis.get_layer(f'G_synthesis/noise{0}').get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(step_noise)):\n",
    "    print(i, step_noise[i].numpy().shape)\n",
    "    stylegan_G.model_synthesis.get_layer(f'G_synthesis/noise{i}').set_weights([step_noise[i].numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(args)\n",
    "id_img, id_z_matching, attr_img, attr_img_indices = dataloader.get_batch(is_train=True, is_cross=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import general_utils as utils\n",
    "\n",
    "sigma = int(80 * (256 / 256))\n",
    "pixel_mask = utils.inverse_gaussian_image(256, sigma)\n",
    "pixel_mask = tf.broadcast_to(pixel_mask, [6, *pixel_mask.shape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_embedding = generator.id_encoder(id_img)\n",
    "src_landmarks = generator.landmarks(attr_img)\n",
    "\n",
    "attr_embedding = generator.attr_encoder(attr_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_embedding = generator.id_encoder(id_img)\n",
    "src_landmarks = generator.landmarks(attr_img)\n",
    "\n",
    "attr_embedding = generator.attr_encoder(attr_img)\n",
    "\n",
    "z_tag = tf.concat([id_embedding, attr_embedding], -1)\n",
    "clatents = generator.reference_mapping(z_tag)\n",
    "\n",
    "gen_img, style_list, _ = generator.stylegan_s(id_z_matching, clatents[:,0,:])\n",
    "\n",
    "# Move to roughly [0,1]\n",
    "gen_img = (gen_img + 1) / 2\n",
    "gen_img = tf.clip_by_value(gen_img, 0, 1)\n",
    "\n",
    "# Identity loss\n",
    "gen_img_id_embedding = generator.id_encoder(gen_img)\n",
    "id_loss = tf.reduce_mean(tf.keras.losses.MAE(gen_img_id_embedding, tf.stop_gradient(id_embedding)))\n",
    "\n",
    "# Landmark loss\n",
    "try:\n",
    "    dst_landmarks = generator.landmarks(gen_img)\n",
    "except Exception as e:\n",
    "    dst_landmarks = None\n",
    "\n",
    "if dst_landmarks is None or src_landmarks is None:\n",
    "    landmarks_loss = 0\n",
    "\n",
    "else:\n",
    "    landmarks_loss = 0.01 * tf.reduce_mean(tf.keras.losses.MSE(src_landmarks, dst_landmarks))\n",
    "\n",
    "# Pixel loss\n",
    "l1_loss = pixel_loss_func(attr_img, gen_img, sample_weight=pixel_mask)\n",
    "mssim = tf.reduce_mean(1 - tf.image.ssim_multiscale(attr_img, gen_img, 1.0))\n",
    "pixel_loss = 0.02 * (0.84 * mssim + 0.16 * l1_loss)\n",
    "\n",
    "# Total loss\n",
    "total_loss = id_loss + landmarks_loss + pixel_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(np.isnan(dst_landmarks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(id_img[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(attr_img[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(gen_img[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def show_landmarks(image, landmarks, bbox=None, retuire_bbox=False):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(image)\n",
    "    ax.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker='.', c='r')\n",
    "    if retuire_bbox:\n",
    "        bbox = bbox[0]\n",
    "        rect = patches.Rectangle((bbox[0], bbox[3] - (bbox[3] - bbox[1])), bbox[2]- bbox[0], bbox[3] - bbox[1], linewidth=1, edgecolor='g', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    plt.pause(0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "id_disen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a53452dba6bf812e948cb4cea9df0bc20a8bff4b740cea6d261396a6414f0665"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
